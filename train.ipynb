{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "all the tokenized words in the json file:\n",
      "[\"'s\", 'a', 'about', 'accept', 'are', 'bye', 'can', 'card', 'cash', 'cert', 'certif', 'credit', 'deliveri', 'do', 'doe', 'done', 'exp', 'experi', 'first', 'forward', 'funni', 'get', 'goodby', 'have', 'hello', 'help', 'hey', 'hi', 'hire', 'hobbi', 'how', 'i', 'industri', 'interview', 'is', 'it', 'joke', 'kind', 'know', 'later', 'like', 'long', 'look', 'lot', 'mani', 'mastercard', 'me', 'meeitng', 'my', 'number', 'of', 'onli', 'pay', 'paypal', 'pleas', 'proj', 'project', 'qualiti', 'schedul', 'see', 'ship', 'should', 'someth', 'strengt', 'strength', 'strenth', 'take', 'tell', 'thank', 'that', 'the', 'to', 'uniq', 'uniqu', 'we', 'what', 'when', 'whi', 'which', 'with', 'would', 'ye', 'year', 'you', 'your', 'yourself', 'zoom']\n",
      "\n",
      "tags from the intents.json file:\n",
      "['certification', 'delivery', 'experience', 'funny', 'goodbye', 'greeting', 'hobbies', 'payments', 'projects', 'thanks', 'unique', 'yes', 'zoom']\n",
      "\n",
      " x_train is [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 1. ... 0. 1. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " y_train is: [ 5  5  5  5 11 11 11 12 12 12 12  4  4  4  9  9  9  9  9  9  0  0  0  0\n",
      "  0  0  7  7  7  7  1  1  1 10 10 10 10 10 10 10 10 10 10  2  2  2  2  2\n",
      "  8  8  8  8  8  6  6  6  6  6  3  3  3  3]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from tokenization_stemming import class_tokenize, class_stemming, class_bag_of_words\n",
    "with open('intents.json', 'r') as f:\n",
    "    object_storedjsonfile = json.load(f)\n",
    "    \n",
    "all_words = [] #patterns from intents.json\n",
    "show_tags = [] #empty list, tags from intents file stored here in the form of array\n",
    "xy = [] #holds patterns and tags from intents.json to analyze patterns and tags\n",
    "\n",
    "for i in object_storedjsonfile['intents']:\n",
    "    obj_tag = i['tag'] #looping through tags\n",
    "    show_tags.append(obj_tag) #stored tag\n",
    "    #within the i, runs the p\n",
    "    for p in i['patterns']:\n",
    "        tokenized_pattern =  class_tokenize(p) #tokenization of pattern, is tokenized pattern\n",
    "        #put tokenized patter in all_words\n",
    "        all_words.extend(tokenized_pattern) #its an array\n",
    "        xy.append((tokenized_pattern, obj_tag))\n",
    "        \n",
    "words_to_ignore = ['?', '!', '.', ',', '{', ':', ';', '\"', '#', '$', '&']\n",
    "all_words = [class_stemming(w) for w in all_words if w not in words_to_ignore]\n",
    "all_words = sorted(set(all_words)) #allwords has tokenized patter,\n",
    "show_tags = sorted(set(show_tags))\n",
    "\n",
    "print(\"\\nall the tokenized words in the json file:\")\n",
    "print(all_words)\n",
    "print(\"\\ntags from the intents.json file:\")\n",
    "print(show_tags)\n",
    "\n",
    "x_train =[] #bag of words\n",
    "y_train = [] \n",
    "\n",
    "for (tokenized_pattern, obj_tag) in xy:\n",
    "    bag = class_bag_of_words(tokenized_pattern, all_words) #patterned_sentence contains tokenized pattern\n",
    "    x_train.append(bag)\n",
    "    y = show_tags.index(obj_tag) #data for y_train\n",
    "    y_train.append(y) #cross ENtRopyloss\n",
    "    \n",
    "#converted to array\n",
    "x_train = np.array(x_train)\n",
    "print(f'\\n x_train is', x_train)\n",
    "y_train = np.array(y_train)\n",
    "print(f'\\n y_train is:', y_train)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first_data is an array using DataLoader: \n",
      "  (array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.], dtype=float32), 5)\n",
      "samples in training, in the class (x_train, y_train) and number of iterations for epoch: \n",
      " 62 8\n",
      " \n",
      " froma above, every epoch has 5 steps\n"
     ]
    }
   ],
   "source": [
    "# creating dataset from x_train and y_train\n",
    "\n",
    "import math\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader #DAtaloader is load bacthes of data for training a model\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "class ChatDataset(Dataset): #__init___ is a constructor, setup for objects\n",
    "    \n",
    "    def __init__(self, x_train, y_train): #data loading\n",
    "        self.n_samples = len(x_train) #initializes n_samples with length of x_train\n",
    "        self.x_data = x_train\n",
    "        self.y_data = y_train\n",
    "\n",
    "    def __getitem__(self, index): #to use index, makes object iterable [dataset 0]\n",
    "       return self.x_data[index], self.y_data[index] #as a tuple\n",
    "\n",
    "    def __len__(self):   #length of the dataset\n",
    "       return self.n_samples\n",
    "\n",
    "    \n",
    "new_created_datset__ = ChatDataset(x_train, y_train)\n",
    "\n",
    "train_loader = DataLoader(dataset= new_created_datset__, batch_size = batch_size, shuffle = True) \n",
    "\n",
    "#shuffle data every epoch. helps in reducing overfitting and data is trained in different order\n",
    "#nn_workers number of subprocesses to use for data loading. multiple cpus, can speed up the data, not needed here\n",
    "\n",
    "first_data = new_created_datset__.__getitem__(0)\n",
    "print(f\"first_data is an array using DataLoader: \\n \", first_data)\n",
    "\n",
    "#printing dataset to get an idea of the training datset\n",
    "total_samples = len(new_created_datset__)\n",
    "n_iter = math.ceil(total_samples/8) # in 1 epoch\n",
    "print(\"samples in training, in the class (x_train, y_train) and number of iterations for epoch: \\n\", total_samples, n_iter)\n",
    "print(\" \\n froma above, every epoch has 5 steps\")\n",
    "\n",
    "#epoch is number of times it will iterate throughout each forward loop\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_size 87 87\n",
      "output_size contains the length of show_tags 13 13 ['certification', 'delivery', 'experience', 'funny', 'goodbye', 'greeting', 'hobbies', 'payments', 'projects', 'thanks', 'unique', 'yes', 'zoom']\n",
      "each epoch = 100/1000, loss=0.6398\n",
      "each epoch = 100/1000, loss=0.2047\n",
      "each epoch = 100/1000, loss=0.7964\n",
      "each epoch = 100/1000, loss=0.8506\n",
      "each epoch = 100/1000, loss=1.1550\n",
      "each epoch = 100/1000, loss=0.6068\n",
      "each epoch = 100/1000, loss=1.1496\n",
      "each epoch = 100/1000, loss=0.4546\n",
      "each epoch = 200/1000, loss=0.0647\n",
      "each epoch = 200/1000, loss=0.0637\n",
      "each epoch = 200/1000, loss=0.3394\n",
      "each epoch = 200/1000, loss=0.1468\n",
      "each epoch = 200/1000, loss=0.0843\n",
      "each epoch = 200/1000, loss=0.0268\n",
      "each epoch = 200/1000, loss=0.0820\n",
      "each epoch = 200/1000, loss=0.0917\n",
      "each epoch = 300/1000, loss=0.0090\n",
      "each epoch = 300/1000, loss=0.0327\n",
      "each epoch = 300/1000, loss=0.0092\n",
      "each epoch = 300/1000, loss=0.0314\n",
      "each epoch = 300/1000, loss=0.0315\n",
      "each epoch = 300/1000, loss=0.0264\n",
      "each epoch = 300/1000, loss=0.0121\n",
      "each epoch = 300/1000, loss=0.0142\n",
      "each epoch = 400/1000, loss=0.0099\n",
      "each epoch = 400/1000, loss=0.0077\n",
      "each epoch = 400/1000, loss=0.0096\n",
      "each epoch = 400/1000, loss=0.0059\n",
      "each epoch = 400/1000, loss=0.0080\n",
      "each epoch = 400/1000, loss=0.0023\n",
      "each epoch = 400/1000, loss=0.0091\n",
      "each epoch = 400/1000, loss=0.0025\n",
      "each epoch = 500/1000, loss=0.0029\n",
      "each epoch = 500/1000, loss=0.0021\n",
      "each epoch = 500/1000, loss=0.0012\n",
      "each epoch = 500/1000, loss=0.0054\n",
      "each epoch = 500/1000, loss=0.0035\n",
      "each epoch = 500/1000, loss=0.0022\n",
      "each epoch = 500/1000, loss=0.0058\n",
      "each epoch = 500/1000, loss=0.0012\n",
      "each epoch = 600/1000, loss=0.0011\n",
      "each epoch = 600/1000, loss=0.0006\n",
      "each epoch = 600/1000, loss=0.0018\n",
      "each epoch = 600/1000, loss=0.0012\n",
      "each epoch = 600/1000, loss=0.0022\n",
      "each epoch = 600/1000, loss=0.0023\n",
      "each epoch = 600/1000, loss=0.0015\n",
      "each epoch = 600/1000, loss=0.0018\n",
      "each epoch = 700/1000, loss=0.0012\n",
      "each epoch = 700/1000, loss=0.0005\n",
      "each epoch = 700/1000, loss=0.0006\n",
      "each epoch = 700/1000, loss=0.0012\n",
      "each epoch = 700/1000, loss=0.0013\n",
      "each epoch = 700/1000, loss=0.0008\n",
      "each epoch = 700/1000, loss=0.0005\n",
      "each epoch = 700/1000, loss=0.0008\n",
      "each epoch = 800/1000, loss=0.0007\n",
      "each epoch = 800/1000, loss=0.0003\n",
      "each epoch = 800/1000, loss=0.0002\n",
      "each epoch = 800/1000, loss=0.0008\n",
      "each epoch = 800/1000, loss=0.0008\n",
      "each epoch = 800/1000, loss=0.0001\n",
      "each epoch = 800/1000, loss=0.0008\n",
      "each epoch = 800/1000, loss=0.0001\n",
      "each epoch = 900/1000, loss=0.0006\n",
      "each epoch = 900/1000, loss=0.0003\n",
      "each epoch = 900/1000, loss=0.0002\n",
      "each epoch = 900/1000, loss=0.0005\n",
      "each epoch = 900/1000, loss=0.0001\n",
      "each epoch = 900/1000, loss=0.0001\n",
      "each epoch = 900/1000, loss=0.0003\n",
      "each epoch = 900/1000, loss=0.0002\n",
      "each epoch = 1000/1000, loss=0.0002\n",
      "each epoch = 1000/1000, loss=0.0002\n",
      "each epoch = 1000/1000, loss=0.0001\n",
      "each epoch = 1000/1000, loss=0.0001\n",
      "each epoch = 1000/1000, loss=0.0003\n",
      "each epoch = 1000/1000, loss=0.0003\n",
      "each epoch = 1000/1000, loss=0.0001\n",
      "each epoch = 1000/1000, loss=0.0001\n",
      "\n",
      " final loss = 0.0001\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "#print(\"nbformat version:\", nbformat.__version__)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#hyperparameters\n",
    "hidden_size = 8\n",
    "output_size = len(show_tags) #number of classes i.e. tags\n",
    "input_size = len(x_train[0]) #length of bag of words, bag of words has same length as all_words #obviously, x[0] as the array of first of bag of words\n",
    "batch_size = 8\n",
    "learning_rate = 0.001\n",
    "number_of_epochs = 1000\n",
    "\n",
    "print(\"input_size\", input_size, len(all_words))\n",
    "print(\"output_size contains the length of show_tags\", output_size, len(show_tags), show_tags)\n",
    "\n",
    "#creating my model using pytorch\n",
    "\n",
    "class classNeuralNet(nn.Module): # defined the basic layers \n",
    "    def __init__(self, input_size, hidden_size, num_classes): #feedforward neural net with 2 layers\n",
    "        # __init__ constructor fn, initializes layers and relu\n",
    "        super(classNeuralNet, self).__init__() #calls constructor from nn.module to ensure initialization\n",
    "        self.l1 = nn.Linear(input_size, hidden_size) #nn.linear accepts inputsize and output size\n",
    "        self.l2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.l3 = nn.Linear( hidden_size, num_classes)\n",
    "        self.relu = nn.ReLU() #activation function, non linenar transformation for a better fit\n",
    "        \n",
    "    def forward (self, x): #to make it functional, specifies how input data passes through network layers\n",
    "        out = self.l1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.l2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.l3(out) #no softmax at the end\n",
    "        \n",
    "        return out\n",
    "        \n",
    "\n",
    "model = classNeuralNet(input_size, hidden_size, output_size).to(device)\n",
    "\n",
    "# loss and optimizer\n",
    "\n",
    "criterion = nn.CrossEntropyLoss() #to predict class prob and evaluate against the actual class label. \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) \n",
    "\n",
    "#training loop\n",
    "for epoch in range(number_of_epochs):\n",
    "    for (words, labels) in train_loader: #words and labels are the data, iterates over bacthes of data provided by trainloader\n",
    "        words = words.to(device) #[70, .......]\n",
    "        labels = labels.to(dtype=torch.long).to(device)\n",
    "        \n",
    "        outputs = model(words) #forward path\n",
    "        loss_obj = criterion(outputs, labels) #predicted outputs and the actual labels\n",
    "        \n",
    "        optimizer.zero_grad() #clear prior gradients\n",
    "        loss_obj.backward() # computes graidients of the loss fn wrt the parameters\n",
    "        optimizer.step() #uses gradient stored in the parameter, updates models parameters\n",
    "        \n",
    "        if (epoch+1) % 100 == 0:\n",
    "            print(f'each epoch = {epoch+1}/{number_of_epochs}, loss={loss_obj.item():.4f}')       \n",
    "\n",
    "print(f'\\n final loss = {loss_obj.item():.4f}') \n",
    "\n",
    "\n",
    "##### 100 samples , batch size of 20 then 100/20 = 5 iterations for 1 epoch\n",
    "# epoch is 1 forward snd backward pass of all training samples ie in entire dataset\n",
    "# iterations are passes, each pass using batch size\n",
    "# batch size is number of training samples in one forward and backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "     \n",
    "#learning_rate = 0.001, \n",
    "# controls step size of each iteration while moving towards a min of loss function\n",
    "#The learning rate affects how much to adjust the weights of the model with respect to the gradient.\n",
    "# A high learning rate can cause the optimizer to overshoot the minimum, \n",
    "# while a low learning rate can result in a slow convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training complete, file saved to FILE\n"
     ]
    }
   ],
   "source": [
    "#save information in data\n",
    "data = {\n",
    "    \"model_state\": model.state_dict(),\n",
    "    \"input_size\": input_size,\n",
    "    \"output_size\": output_size,\n",
    "    \"all_words\": all_words, \n",
    "    \"hidden_size\": hidden_size,\n",
    "    \"show_tags\": show_tags,\n",
    "}\n",
    "\n",
    "FILE = \"data.pth\"\n",
    "torch.save(data, FILE) # saves data\n",
    "\n",
    "print(f\"training complete, file saved to FILE\")\n",
    "#print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/15/jg43fpfx5_lbw1bx2_6xgszw0000gn/T/ipykernel_97757/3121144163.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data_file = torch.load(FILE)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "For ease of reference, you can use 'Skay' to address me.\n",
      "\n",
      "\n",
      " Skay:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       " \n",
       "First born daughter, strong yet empathetic. \n",
       "Punctuality is a virtue & one of my key strengths. \n",
       "Compliments often heard - 'beautiful listener'.  \n",
       "Adherence to balanced fitness regimen, criticial in sustaining a strong work ethic & optimal performance, plus a bit of an OCD.  \n",
       "I prioritize professionalism.  \n",
       "Emotionally intelligent through diverse experiences, enhancing my ability to navigate complex interpersonal dynamics."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#final chat interface\n",
    "\n",
    "import random\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "\n",
    "with open('intents.json', 'r') as f:\n",
    "    obj1 = json.load(f)\n",
    "     \n",
    "data_file = torch.load(FILE)\n",
    "\n",
    "input_size = data[\"input_size\"] \n",
    "output_size = data[\"output_size\"]\n",
    "all_words = data[\"all_words\"]\n",
    "model_state = data[\"model_state\"]\n",
    "hidden_size = data[\"hidden_size\"]\n",
    "show_tags = data[\"show_tags\"]\n",
    "\n",
    "model.load_state_dict(model_state) #learnt parameters\n",
    "model.eval()\n",
    "\n",
    "# chat loop \n",
    "bot_name =  \" Skay\"\n",
    "print(\"\\nFor ease of reference, you can use 'Skay' to address me.\")\n",
    "\n",
    "# first way to implement chatbot.\n",
    "print(\"\\n Skay: \\nPlease enter your question. To exit, type 'quit' in lowercase.\")\n",
    "\n",
    "while True:\n",
    "    sentence = input('You: ')\n",
    "    if sentence == \"quit\":\n",
    "        break\n",
    "    sentence = class_tokenize(sentence)\n",
    "    x = class_bag_of_words(sentence, all_words)\n",
    "    x = x.reshape(1, x.shape[0]) #0 is 0 number of columns\n",
    "    x = torch.from_numpy(x) #tensor\n",
    "    \n",
    "    obj_modeloutput = model(x)\n",
    "    _,predicted = torch.max(obj_modeloutput, dim=1)\n",
    "    tag = show_tags[predicted.item()] # tag is from intents.json, predicted.item() is class label\n",
    "    \n",
    "    probs = torch.softmax(obj_modeloutput, dim=1)\n",
    "    prob = probs[0][predicted.item()]\n",
    "    \n",
    "    if prob.item() > 0.75:\n",
    "        for i in obj1[\"intents\"]:\n",
    "            if tag  == i[\"tag\"]:\n",
    "                print(\"\\n\")\n",
    "                print(f\"{bot_name}:\")\n",
    "                display(Markdown(random.choice(i['responses'])))\n",
    "                \n",
    "    else:\n",
    "        print (f\"{bot_name}: I do not understand, could you please repeat\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Skay: \n",
      "Please enter your question. To exit, type 'quit' in lowercase.\n"
     ]
    }
   ],
   "source": [
    "# 2nd way\n",
    "# using function as it's an efficient way to integrate and deploy to a webpage\n",
    "\n",
    "bot_name =  \" Skay\"\n",
    "\n",
    "def chat_response(msg):\n",
    "    sentence = class_tokenize(msg)\n",
    "    X = class_bag_of_words(sentence, all_words)\n",
    "    X = X.reshape(1, X.shape[0])\n",
    "    X = torch.from_numpy(X).to(device)\n",
    "\n",
    "    output = model(X)\n",
    "    _, predicted = torch.max(output, dim=1)\n",
    "\n",
    "    tag = show_tags[predicted.item()]\n",
    "\n",
    "    probs = torch.softmax(output, dim=1)\n",
    "    prob = probs[0][predicted.item()]\n",
    "    if prob.item() > 0.75:\n",
    "        for k in obj1['intents']:\n",
    "            if tag == k[\"tag\"]:\n",
    "                return random.choice(k['responses'])\n",
    "    \n",
    "    return \"I do not understand...\"\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n Skay: \\nPlease enter your question. To exit, type 'quit' in lowercase.\")\n",
    "    while True:\n",
    "        # sentence = \"do you use credit cards?\"\n",
    "        sentence = input(\"You: \")\n",
    "        if sentence == \"quit\":\n",
    "            break\n",
    "\n",
    "        resp = chat_response(sentence)\n",
    "        print(\"\\n\")\n",
    "        print(f\"{bot_name}:\", resp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
